{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install captum","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:37:11.502391Z","iopub.execute_input":"2022-11-20T05:37:11.503408Z","iopub.status.idle":"2022-11-20T05:37:23.326695Z","shell.execute_reply.started":"2022-11-20T05:37:11.503317Z","shell.execute_reply":"2022-11-20T05:37:23.325548Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting captum\n  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from captum) (1.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from captum) (1.21.6)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from captum) (3.5.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->captum) (4.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (2.8.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (1.4.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (9.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (4.33.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (21.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.15.0)\nInstalling collected packages: captum\nSuccessfully installed captum-0.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nimport torch\nfrom datasets import Dataset\nimport transformers\nfrom transformers import (\n  AdamW,\n  BertConfig,\n  BertModel,\n  BertTokenizer,\n  DistilBertTokenizer,\n  DistilBertModel,\n  DistilBertForSequenceClassification,\n  BertForSequenceClassification)\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport os\nimport captum\nfrom captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\nfrom captum.attr import visualization as viz\nfrom captum.attr import LayerConductance, LayerIntegratedGradients\n\nimport numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix\n\nfrom IPython.display import display, HTML\nimport matplotlib as mpl\nfrom matplotlib.colors import Normalize, rgb2hex\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-11-20T06:06:39.050327Z","iopub.execute_input":"2022-11-20T06:06:39.050713Z","iopub.status.idle":"2022-11-20T06:06:39.058225Z","shell.execute_reply.started":"2022-11-20T06:06:39.050681Z","shell.execute_reply":"2022-11-20T06:06:39.057254Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"finegrained_sentiments_dict = {\n\"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n\"disgust\": [\"disgust\"],\n\"fear\": [\"fear\", \"nervousness\"],\n\"joy\": [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",  \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"],\n\"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n\"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]\n}","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:37:45.752065Z","iopub.execute_input":"2022-11-20T05:37:45.753036Z","iopub.status.idle":"2022-11-20T05:37:45.759376Z","shell.execute_reply.started":"2022-11-20T05:37:45.752997Z","shell.execute_reply":"2022-11-20T05:37:45.758288Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!ls ../","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:37:47.371342Z","iopub.execute_input":"2022-11-20T05:37:47.371695Z","iopub.status.idle":"2022-11-20T05:37:48.315472Z","shell.execute_reply.started":"2022-11-20T05:37:47.371665Z","shell.execute_reply":"2022-11-20T05:37:48.314303Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"input  lib  working\n","output_type":"stream"}]},{"cell_type":"code","source":"DATA_DIR = \"../input/emotionclss/\"\ntrain = {\"input\": [], \"labels\": []}\ndev = {\"input\": [], \"labels\": []}\ntest = {\"input\": [], \"labels\": []}\n\nwith open(DATA_DIR + \"train.tsv\") as file:\n    tsv_file = csv.reader(file, delimiter=\"\\t\") \n    for line in tsv_file:\n        train[\"input\"].append(line[0])\n        labels = line[1].split(\",\")\n        one_hot = [0 for i in range(28)]\n        for label in labels:\n            one_hot[int(label)] = 1\n        train[\"labels\"].append(one_hot)\n\nwith open(DATA_DIR + \"dev.tsv\") as file:\n    tsv_file = csv.reader(file, delimiter=\"\\t\") \n    for line in tsv_file:\n        dev[\"input\"].append(line[0])\n        labels = line[1].split(\",\")\n        one_hot = [0 for i in range(28)]\n        for label in labels:\n            one_hot[int(label)] = 1\n        dev[\"labels\"].append(one_hot)\n\nwith open(DATA_DIR + \"test.tsv\") as file:\n    tsv_file = csv.reader(file, delimiter=\"\\t\") \n    for line in tsv_file:\n        test[\"input\"].append(line[0])\n        labels = line[1].split(\",\")\n        one_hot = [0 for i in range(28)]\n        for label in labels:\n            one_hot[int(label)] = 1\n        test[\"labels\"].append(one_hot)\n        \nprint(\"Number of train examples are {}\".format(len(train[\"input\"])))\nprint(\"Number of dev examples are {}\".format(len(dev[\"input\"])))\nprint(\"Number of test examples are {}\".format(len(test[\"input\"])))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:38:14.697502Z","iopub.execute_input":"2022-11-20T05:38:14.698181Z","iopub.status.idle":"2022-11-20T05:38:15.182844Z","shell.execute_reply.started":"2022-11-20T05:38:14.698137Z","shell.execute_reply":"2022-11-20T05:38:15.180982Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of train examples are 43410\nNumber of dev examples are 5426\nNumber of test examples are 5427\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating higgingface datasets\ntrain_dataset = Dataset.from_dict(train)\ndev_dataset = Dataset.from_dict(dev)\ntest_dataset = Dataset.from_dict(test)\n\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:38:18.041996Z","iopub.execute_input":"2022-11-20T05:38:18.042751Z","iopub.status.idle":"2022-11-20T05:38:18.267605Z","shell.execute_reply.started":"2022-11-20T05:38:18.042713Z","shell.execute_reply":"2022-11-20T05:38:18.266522Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input', 'labels'],\n    num_rows: 43410\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"class LoadData(torch.utils.data.Dataset):\n    \"\"\"\n    Using this since dataloader expects map-style dataset objects\n    \n    \"\"\"\n    \n    def __init__(\n        self, dataset, tokenizer, source_length):\n        \"\"\"\n        Initializes a Dataset class\n\n        Args:\n            dataset (Dataset object): Input Dataset\n            tokenizer (Tokenizer object): Transformer tokenizer\n            source_length (int): Max length of source text\n        \"\"\"\n        \n        self.tokenizer = tokenizer\n        self.data = dataset\n        self.source_length = source_length\n        self.source_text = self.data[\"input\"]\n        self.target_labels = self.data[\"labels\"]\n\n    def __len__(self):\n        return len(self.target_labels)\n\n    def __getitem__(self, index):\n        \"\"\"\n        return input ids, attention masks and target ids\n        \n        \"\"\"\n        source_text = str(self.source_text[index])\n        target_label = self.target_labels[index]\n\n        # cleaning data so as to ensure data is in string type\n        source_text = \" \".join(source_text.split())\n\n        source = self.tokenizer.__call__(\n            [source_text],\n            max_length=self.source_length,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        \n        target = torch.tensor(target_label)\n\n        source_ids = source[\"input_ids\"].squeeze()\n        source_mask = source[\"attention_mask\"].squeeze()\n\n        return {\n            \"source_ids\": source_ids.to(dtype=torch.long),\n            \"source_mask\": source_mask.to(dtype=torch.long),\n            \"target\": target.squeeze().to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:38:20.916463Z","iopub.execute_input":"2022-11-20T05:38:20.917209Z","iopub.status.idle":"2022-11-20T05:38:20.927937Z","shell.execute_reply.started":"2022-11-20T05:38:20.917170Z","shell.execute_reply":"2022-11-20T05:38:20.926730Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#joeddav/distilbert-base-uncased-go-emotions-student\nparameters = {\"model\": \"bhadresh-savani/bert-base-go-emotion\",  # model_type: t5-base/t5-large\n    \"train_bs\": 8,  # training batch size\n    \"val_bs\": 10,  # validation batch size\n    \"test_bs\": 15,\n    \"epochs\": 3,  # number of training epochs\n    \"lr\": 6e-4,  # learning rate\n    \"wd\": 0.0001,\n    \"max_source_length\": 512,  # max length of source text\n    \"SEED\": 42,\n    \"out_dir\": \"./\",\n    \"hidden_size\": 768,\n    \"num_classes\": 28}\n\nindex_label = {0:\"admiration\", 1:\"amusement\", 2:\"anger\", 3:\"annoyance\", 4:\"approval\", 5:\"caring\", 6:\"confusion\",\n            7:\"curiosity\", 8:\"desire\", 9:\"disappointment\", 10:\"disapproval\", 11:\"disgust\", 12:\"embarrassment\",\n            13:\"excitement\", 14:\"fear\", 15:\"gratitude\", 16:\"grief\", 17:\"joy\", 18:\"love\", 19:\"nervousness\",\n            20:\"optimism\", 21:\"pride\", 22:\"realization\", 23:\"relief\", 24:\"remorse\", 25:\"sadness\",\n            26:\"surprise\", 27:\"neutral\"}\nlabel_list = list(index_label.values())","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:38:25.732665Z","iopub.execute_input":"2022-11-20T05:38:25.733757Z","iopub.status.idle":"2022-11-20T05:38:25.742020Z","shell.execute_reply.started":"2022-11-20T05:38:25.733713Z","shell.execute_reply":"2022-11-20T05:38:25.740646Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_metrics_allemotions(outputs, labels, label_list_, index_label):\n    predictions = []\n    \n    for output in outputs:\n        output = [int(out > 0.32) for out in output]\n        predictions.append(output)\n    print(\"1st prediction\", predictions[0])\n    \n    confusion_matrix = {}\n    precisions, recalls, fscores = {}, {}, {}\n    for label in label_list_:\n        confusion_matrix[label] = {\"TP\":0, \"FP\": 0, \"FN\": 0}\n        precisions[label], recalls[label], fscores[label] = 0, 0, 0\n    \n    for i, prediction in enumerate(predictions):\n        gt = labels[i]\n        for j, out in enumerate(gt):\n            pred = prediction[j]\n            if out == 0 and pred == 0: continue\n            elif out == 0 and pred == 1:\n                # FP found\n                confusion_matrix[index_label[j]][\"FP\"] += 1\n            elif out == 1 and pred == 0:\n                # FN found\n                confusion_matrix[index_label[j]][\"FN\"] += 1\n            elif out == 1 and pred == 1:\n                # TP found\n                confusion_matrix[index_label[j]][\"TP\"] += 1\n    \n    \n    for label in label_list_:\n        precisions[label] = confusion_matrix[label][\"TP\"]/(confusion_matrix[label][\"TP\"] + confusion_matrix[label][\"FP\"] + 1e-4)\n        recalls[label] = confusion_matrix[label][\"TP\"]/(confusion_matrix[label][\"TP\"] + confusion_matrix[label][\"FN\"] + 1e-4)\n        fscores[label] = 2*precisions[label]*recalls[label]/(precisions[label]+recalls[label] + 1e-4)\n    \n    return precisions, recalls, fscores\n\ndef compute_test_outputs(model, test_dataloader, tokenizer, device, label_list, index_label):\n    predictions = []\n    labels = []\n    \n    with torch.no_grad():\n        steps = 0\n        for test_batch in test_dataloader:\n            y = test_batch['target'].to(device, dtype = torch.float32)\n            ids = test_batch['source_ids'].to(device, dtype = torch.long)\n            mask = test_batch['source_mask'].to(device, dtype = torch.long)\n\n            output = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n            \n            output = output[\"logits\"]\n            output = torch.sigmoid(output)\n            \n            predictions.extend(output.detach().cpu().numpy())\n            labels.extend(y.detach().cpu().numpy())\n#             if steps == 5: break\n    \n    return predictions, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:38:32.049466Z","iopub.execute_input":"2022-11-20T05:38:32.049956Z","iopub.status.idle":"2022-11-20T05:38:32.072784Z","shell.execute_reply.started":"2022-11-20T05:38:32.049918Z","shell.execute_reply":"2022-11-20T05:38:32.071559Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"cuda =  torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n\ntokenizer = DistilBertTokenizer.from_pretrained(parameters[\"model\"])\nmodel = BertForSequenceClassification.from_pretrained(parameters[\"model\"])\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:38:35.321684Z","iopub.execute_input":"2022-11-20T05:38:35.322117Z","iopub.status.idle":"2022-11-20T05:39:05.514030Z","shell.execute_reply.started":"2022-11-20T05:38:35.322078Z","shell.execute_reply":"2022-11-20T05:39:05.513039Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65af0f0067f24accbbc6f9d2f1afe1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fed4c307f104c708a91495ffda2cd42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/333 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c0bc821cdf40e58f2767ba43d62de5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b712f9a5f11a4fee883c27da67b877f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcddee359efb47a4bcbe50cdfe90fe35"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Computing test data Fscores","metadata":{}},{"cell_type":"code","source":"test_obj = LoadData(\n        test_dataset,\n        tokenizer,\n        parameters[\"max_source_length\"]\n    )\ntest_loader = DataLoader(test_obj, shuffle=True, batch_size=parameters[\"test_bs\"])\npredictions, labels = compute_test_outputs(model, test_loader, tokenizer, device, label_list, index_label)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T10:07:38.292590Z","iopub.execute_input":"2022-11-16T10:07:38.293027Z","iopub.status.idle":"2022-11-16T10:09:13.615635Z","shell.execute_reply.started":"2022-11-16T10:07:38.292985Z","shell.execute_reply":"2022-11-16T10:09:13.611305Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"precisions, recalls, fscores = compute_metrics_allemotions(predictions, labels, label_list, index_label)\nprint(\"Precision, Recall and Fscores for all labels are \")\n\nprecision, recall, fscore = 0, 0, 0\nfor label in label_list:\n    precision += precisions[label]\n    recall += recalls[label]\n    fscore += fscores[label]\n    print(\"Emotion {}: precision: {}, recall: {}, fscore: {}\".format(label, precisions[label], \n                                                                     recalls[label], fscores[label]))\n\nprint(\"Macro precision: {}, Macro recall: {}, Macro fscore: {}\".format(precision/28, recall/28, fscore/28))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T10:09:47.937603Z","iopub.execute_input":"2022-11-16T10:09:47.937970Z","iopub.status.idle":"2022-11-16T10:09:48.483976Z","shell.execute_reply.started":"2022-11-16T10:09:47.937940Z","shell.execute_reply":"2022-11-16T10:09:48.482828Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1st prediction [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nPrecision, Recall and Fscores for all labels are \nEmotion admiration: precision: 0.7300612003964826, recall: 0.7083331927910332, fscore: 0.7189831026922668\nEmotion amusement: precision: 0.7641507030972632, recall: 0.9204541967976528, fscore: 0.8350016928135315\nEmotion anger: precision: 0.5902773678629389, recall: 0.42929271247842804, fscore: 0.49702698402148593\nEmotion annoyance: precision: 0.5662643779947253, recall: 0.14687495410157686, fscore: 0.23321780161193786\nEmotion approval: precision: 0.583332928241022, recall: 0.23931617113499398, fscore: 0.3393525510741509\nEmotion caring: precision: 0.5396816830449476, recall: 0.25185166529506275, fscore: 0.343390613569084\nEmotion confusion: precision: 0.6206885850196809, recall: 0.23529396386015433, fscore: 0.34119204434929035\nEmotion curiosity: precision: 0.5342463923813725, recall: 0.5492955812339503, fscore: 0.5416164928462633\nEmotion desire: precision: 0.7234027161644336, recall: 0.4096380606770353, fscore: 0.5230299567262977\nEmotion disappointment: precision: 0.4999950000499995, recall: 0.033112560852608705, fscore: 0.062100075389515036\nEmotion disapproval: precision: 0.5555547839516889, recall: 0.1498126779727798, fscore: 0.23595461006053486\nEmotion disgust: precision: 0.6842093259485509, recall: 0.31707291294885126, fscore: 0.4332895783814763\nEmotion embarrassment: precision: 0.5999880002399952, recall: 0.08108086194361636, fscore: 0.14283549033447268\nEmotion excitement: precision: 0.6491216682075996, recall: 0.359222952210726, fscore: 0.4624535592262016\nEmotion fear: precision: 0.71999904000128, recall: 0.6923068047348657, fscore: 0.7058314529823759\nEmotion gratitude: precision: 0.9499997205883175, recall: 0.9176133756780184, fscore: 0.9334757594674072\nEmotion grief: precision: 0.0, recall: 0.0, fscore: 0.0\nEmotion joy: precision: 0.6474354824131523, recall: 0.6273288029013646, fscore: 0.6371735890897742\nEmotion love: precision: 0.7545784781763817, recall: 0.865545854812666, fscore: 0.8062121529942877\nEmotion nervousness: precision: 0.0, recall: 0.0, fscore: 0.0\nEmotion optimism: precision: 0.6870743625344472, recall: 0.543010460747064, fscore: 0.606556932107346\nEmotion pride: precision: 0.0, recall: 0.0, fscore: 0.0\nEmotion realization: precision: 0.8333194446759221, recall: 0.03448273483949321, fscore: 0.06621744737038807\nEmotion relief: precision: 0.0, recall: 0.0, fscore: 0.0\nEmotion remorse: precision: 0.6027389003576707, recall: 0.7857128826555666, fscore: 0.6821203568814359\nEmotion sadness: precision: 0.6944438014409245, recall: 0.480768922584024, fscore: 0.5681330447449834\nEmotion surprise: precision: 0.6565649933686936, recall: 0.4609925808563256, fscore: 0.5416177508622388\nEmotion neutral: precision: 0.7112117377569798, recall: 0.7028539058279851, fscore: 0.7069581276009392\nMacro precision: 0.567797881925517, Macro recall: 0.3907595996405658, Macro fscore: 0.4272764702570601\n","output_type":"stream"}]},{"cell_type":"code","source":"ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\nsep_token_id = tokenizer.sep_token_id # A token used as a separator at the end of the text.\ncls_token_id = tokenizer.cls_token_id # A token used for prepending to the word sequence","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:39:25.421955Z","iopub.execute_input":"2022-11-20T05:39:25.422296Z","iopub.status.idle":"2022-11-20T05:39:25.427501Z","shell.execute_reply.started":"2022-11-20T05:39:25.422267Z","shell.execute_reply":"2022-11-20T05:39:25.426612Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device):\n    text_ids = tokenizer.encode(text, add_special_tokens=False)\n\n    # construct input token ids\n    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n\n    # construct reference token ids \n    ref_input_ids = [cls_token_id] + [ref_token_id] * (len(input_ids)-2) + [sep_token_id]\n\n    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(input_ids)\n\ndef construct_attention_mask(input_ids):\n    return torch.ones_like(input_ids)\n\ndef predict(inputs, attention_mask=None):\n    output = model(inputs, attention_mask=attention_mask, )\n    output = output[\"logits\"]\n    output = torch.sigmoid(output)\n    return output\n\ndef forward_func(inputs, i, device, attention_mask=None):\n    pred = predict(inputs,\n                   attention_mask=attention_mask)\n    #return pred.max(1).values\n    pred = torch.index_select(pred, 1, torch.tensor([i], device=device))\n    return pred\n\ndef summarize_attributions(attributions):\n    attributions = attributions.sum(dim=-1).squeeze(0)\n    attributions = attributions / torch.norm(attributions)\n    return attributions","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:39:28.327961Z","iopub.execute_input":"2022-11-20T05:39:28.328458Z","iopub.status.idle":"2022-11-20T05:39:28.375536Z","shell.execute_reply.started":"2022-11-20T05:39:28.328414Z","shell.execute_reply":"2022-11-20T05:39:28.373149Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# preparing data\ndata = []\nfor i,text in enumerate(test_dataset[\"input\"][0:100]):\n    out = test_dataset[\"labels\"][i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    #print(len(out[0]), len(input_ids[0]), len(ref_input_ids[0]), len(attention_mask[0]))\n    data.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T12:53:09.115690Z","iopub.execute_input":"2022-11-14T12:53:09.116040Z","iopub.status.idle":"2022-11-14T12:53:14.827099Z","shell.execute_reply.started":"2022-11-14T12:53:09.116012Z","shell.execute_reply":"2022-11-14T12:53:14.826108Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def colorize(attrs, cmap='PiYG'):\n\n    cmap_bound = max([abs(attr) for attr in attrs])\n\n    norm = Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n\n    cmap = mpl.cm.get_cmap(cmap)\n    colors = list(map(lambda x: rgb2hex(cmap(norm(x))), attrs))\n\n    return colors\n\ndef  hlstr(string, color='white'):\n    return f\"<mark style=background-color:{color}>{string} </mark>\"    \n    \ndef explainability(data, limit):\n    lig = LayerIntegratedGradients(forward_func, model.bert.embeddings)  # embeddings is the first layer\n    attributions = []\n    predictions = []\n    for j,sample in enumerate(data[0:limit]):\n        pred = predict(sample[1], attention_mask=sample[3])\n\n        classes = []\n        for i in range(0,28):\n            attribution, delta = lig.attribute(inputs=sample[1],\n                                          baselines=sample[2],\n                                          additional_forward_args=(i, device, sample[3]),\n                                          return_convergence_delta=True)\n            attribution = summarize_attributions(attribution).detach().cpu().numpy()\n            attribution = [(attr,k) for k,attr in enumerate(attribution)]\n            attribution.sort(key=(lambda x: x[0]), reverse=True)\n            classes.append(attribution)\n        attributions.append(classes)\n        predictions.append(pred)\n    return attributions, predictions\n\ndef color(word_scores, words):\n    colors = colorize(word_scores)\n    colored_input = []\n    display(HTML(\"\".join(list(map(hlstr, words, colors)))))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T06:11:26.596632Z","iopub.execute_input":"2022-11-20T06:11:26.596973Z","iopub.status.idle":"2022-11-20T06:11:26.607639Z","shell.execute_reply.started":"2022-11-20T06:11:26.596946Z","shell.execute_reply":"2022-11-20T06:11:26.606692Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"Predicted label is:\", index_label[np.argmax(predictions[3][0].detach().cpu().numpy())], np.argmax(predictions[3][0].detach().cpu().numpy()))\nprint(\"printing more valuable 3 words for each class:\")\nfor i in range(28):\n    print(\"Now we have class {}!!!!\".format(index_label[i]))\n    words = []\n    for k in range(5):\n        words.append(data[3][-1][attributions[3][i][k][1]])\n    print(tuple(words))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:26:20.631161Z","iopub.execute_input":"2022-11-14T11:26:20.631520Z","iopub.status.idle":"2022-11-14T11:26:20.641795Z","shell.execute_reply.started":"2022-11-14T11:26:20.631489Z","shell.execute_reply":"2022-11-14T11:26:20.640729Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Predicted label is: gratitude 15\nprinting more valuable 3 words for each class:\nNow we have class admiration!!!!\n(',', \"'\", '!', 'thank', 'today')\nNow we have class amusement!!!!\n('i', '!', 'me', \"'\", ',')\nNow we have class anger!!!!\n('t', 'me', 'didn', 'teaching', '!')\nNow we have class annoyance!!!!\n('t', 'didn', 'me', \"'\", '!')\nNow we have class approval!!!!\n(\"'\", 'know', 'you', 'today', ',')\nNow we have class caring!!!!\n('you', 'thank', \"'\", ',', 'for')\nNow we have class confusion!!!!\n('know', 'didn', 't', 'something', 'i')\nNow we have class curiosity!!!!\n('know', 'didn', 'something', 'me', 'for')\nNow we have class desire!!!!\n('i', 'something', 'for', 'you', 'today')\nNow we have class disappointment!!!!\n('t', 'didn', 'me', 'teaching', 'for')\nNow we have class disapproval!!!!\n('t', \"'\", 'didn', 'teaching', '[CLS]')\nNow we have class disgust!!!!\n('t', 'didn', 'me', \"'\", 'teaching')\nNow we have class embarrassment!!!!\n('didn', 'me', 'i', 't', 'teaching')\nNow we have class excitement!!!!\n('!', 'me', 'something', 'know', \"'\")\nNow we have class fear!!!!\n('me', 'something', 'for', 'didn', 'teaching')\nNow we have class gratitude!!!!\n('thank', \"'\", '!', 'know', 'you')\nNow we have class grief!!!!\n('me', 'for', 'teaching', 'that', ',')\nNow we have class joy!!!!\n('!', \"'\", 'me', 'something', 'know')\nNow we have class love!!!!\n('!', ',', 'me', 'today', \"'\")\nNow we have class nervousness!!!!\n('me', 'something', 'didn', 'for', 'i')\nNow we have class optimism!!!!\n('you', 'something', 'know', \"'\", 'today')\nNow we have class pride!!!!\n(\"'\", ',', '!', 'today', 'you')\nNow we have class realization!!!!\n('know', 't', 'i', \"'\", 'didn')\nNow we have class relief!!!!\n(\"'\", '!', ',', 'me', 'you')\nNow we have class remorse!!!!\n('didn', 'i', 't', 'me', 'teaching')\nNow we have class sadness!!!!\n('me', 'for', 'teaching', 'didn', 'that')\nNow we have class surprise!!!!\n('t', '!', 'i', 'me', 'know')\nNow we have class neutral!!!!\n('today', '!', 'know', 'teaching', '[CLS]')\n","output_type":"stream"}]},{"cell_type":"code","source":"start_position_vis = viz.VisualizationDataRecord(\n                        attributions_start_sum,\n                        torch.max(pred),\n                        torch.argmax(pred),\n                        torch.argmax(pred),\n                        str(14),\n                        attributions_start_sum.sum(),\n                        all_tokens,\n                        delta_start)\n\nprint('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\nviz.visualize_text([start_position_vis])","metadata":{"execution":{"iopub.status.busy":"2022-11-09T17:31:08.446372Z","iopub.execute_input":"2022-11-09T17:31:08.447045Z","iopub.status.idle":"2022-11-09T17:31:08.465652Z","shell.execute_reply.started":"2022-11-09T17:31:08.447009Z","shell.execute_reply":"2022-11-09T17:31:08.464631Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[1m Visualizations For Start Position \u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>20</b></text></td><td><text style=\"padding-right:2em\"><b>20 (0.42)</b></text></td><td><text style=\"padding-right:2em\"><b>14</b></text></td><td><text style=\"padding-right:2em\"><b>1.40</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fan                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> good                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luck                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> guys                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interesting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watch                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>20</b></text></td><td><text style=\"padding-right:2em\"><b>20 (0.42)</b></text></td><td><text style=\"padding-right:2em\"><b>14</b></text></td><td><text style=\"padding-right:2em\"><b>1.40</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fan                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> good                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luck                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> guys                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interesting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watch                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Analyzing Grief","metadata":{}},{"cell_type":"code","source":"grief = {\"input\":[], \"labels\":[]}\nfor i,text in enumerate(test_dataset[\"input\"]):\n    out = test_dataset[\"labels\"][i]\n    if out[16] == 1:\n        grief[\"input\"].append(text)\n        grief[\"labels\"].append(out)\n\nprint(len(grief[\"input\"]))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T12:58:12.941336Z","iopub.execute_input":"2022-11-14T12:58:12.941715Z","iopub.status.idle":"2022-11-14T13:03:39.826507Z","shell.execute_reply.started":"2022-11-14T12:58:12.941676Z","shell.execute_reply":"2022-11-14T13:03:39.825414Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"6\n","output_type":"stream"}]},{"cell_type":"code","source":"data_grief = []\nfor i,text in enumerate(grief[\"input\"]):\n    out = grief[\"labels\"][i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    data_grief.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:03:50.511127Z","iopub.execute_input":"2022-11-14T13:03:50.511488Z","iopub.status.idle":"2022-11-14T13:03:50.524444Z","shell.execute_reply.started":"2022-11-14T13:03:50.511460Z","shell.execute_reply":"2022-11-14T13:03:50.523576Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"attributions, predictions = explainability(data_grief, 10)\npredictions = [pred.detach().cpu().numpy()[0] > 0.32 for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:14:42.550387Z","iopub.execute_input":"2022-11-14T13:14:42.550738Z","iopub.status.idle":"2022-11-14T13:15:01.848932Z","shell.execute_reply.started":"2022-11-14T13:14:42.550710Z","shell.execute_reply":"2022-11-14T13:15:01.847848Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"for k, pred in enumerate(predictions):\n    classes = [index_label[index] for index,i in enumerate(pred) if i]\n    print(\"predicted classes are {}\".format(classes))\n    print(\"Input text is {}\".format(grief[\"input\"][k]))\n    print(\"Important words are:\")\n    classes = [index for index,i in enumerate(pred) if i]\n    words = []\n    for index in classes:\n        for m in range(6):\n            words.append(data_grief[k][-1][attributions[k][index][m][1]])\n    print(tuple(words))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:25:24.920376Z","iopub.execute_input":"2022-11-14T13:25:24.920731Z","iopub.status.idle":"2022-11-14T13:25:24.928994Z","shell.execute_reply.started":"2022-11-14T13:25:24.920702Z","shell.execute_reply":"2022-11-14T13:25:24.928052Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"predicted classes are ['curiosity']\nInput text is [NAME] death is just so..... senseless. Why? WHY??? The based gods have forsaken us\nImportant words are:\n('?', '?', '?', '?', 'why', 'name')\npredicted classes are ['sadness']\nInput text is Rip the guy from psych\nImportant words are:\n('rip', '##ych', 'the', 'guy', '[CLS]', '[SEP]')\npredicted classes are ['sadness']\nInput text is The only death that made me feel any emotion. And it wasn’t even the death itself.\nImportant words are:\n('death', 'death', 'feel', 'me', 'made', 'even')\npredicted classes are ['sadness']\nInput text is My condolences.\nImportant words are:\n('##lence', 'condo', 'my', '##s', '[CLS]', '[SEP]')\npredicted classes are ['sadness']\nInput text is Oh my gosh. This woman who died also had a son who died. Holy tragedy\nImportant words are:\n('died', 'died', 'son', 'go', 'my', 'woman')\npredicted classes are ['sadness']\nInput text is You'll miss a begging old man asking for a spare coin. RIP\nImportant words are:\n('rip', 'miss', 'old', 'spare', 'for', 'begging')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Analyzing pride","metadata":{}},{"cell_type":"code","source":"pride = {\"input\":[], \"labels\":[]}\nfor i,text in enumerate(test_dataset[\"input\"]):\n    out = test_dataset[\"labels\"][i]\n    if out[21] == 1:\n        pride[\"input\"].append(text)\n        pride[\"labels\"].append(out)\n\nprint(len(pride[\"input\"]))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:29:58.041640Z","iopub.execute_input":"2022-11-14T13:29:58.042005Z","iopub.status.idle":"2022-11-14T13:35:20.774493Z","shell.execute_reply.started":"2022-11-14T13:29:58.041977Z","shell.execute_reply":"2022-11-14T13:35:20.773370Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"16\n","output_type":"stream"}]},{"cell_type":"code","source":"data_pride = []\nfor i,text in enumerate(pride[\"input\"]):\n    out = pride[\"labels\"][i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    data_pride.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:35:52.706392Z","iopub.execute_input":"2022-11-14T13:35:52.706750Z","iopub.status.idle":"2022-11-14T13:35:52.726965Z","shell.execute_reply.started":"2022-11-14T13:35:52.706721Z","shell.execute_reply":"2022-11-14T13:35:52.726116Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"attributions, predictions = explainability(data_pride, 16)\npredictions = [pred.detach().cpu().numpy()[0] > 0.32 for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:36:03.516317Z","iopub.execute_input":"2022-11-14T13:36:03.516683Z","iopub.status.idle":"2022-11-14T13:36:57.497937Z","shell.execute_reply.started":"2022-11-14T13:36:03.516647Z","shell.execute_reply":"2022-11-14T13:36:57.496970Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"for k, pred in enumerate(predictions):\n    classes = [index_label[index] for index,i in enumerate(pred) if i]\n    print(\"predicted classes are {}\".format(classes))\n    print(\"Input text is {}\".format(pride[\"input\"][k]))\n    print(\"Important words are:\")\n    classes = [index for index,i in enumerate(pred) if i]\n    words = []\n    for index in classes:\n        for m in range(6):\n            words.append(data_pride[k][-1][attributions[k][index][m][1]])\n    print(tuple(words))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T13:40:17.241932Z","iopub.execute_input":"2022-11-14T13:40:17.242595Z","iopub.status.idle":"2022-11-14T13:40:17.253337Z","shell.execute_reply.started":"2022-11-14T13:40:17.242559Z","shell.execute_reply":"2022-11-14T13:40:17.252124Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"predicted classes are ['neutral']\nInput text is I only eat cronuts cuz I'm sophisticated!\nImportant words are:\n('sophisticated', 'only', '##z', 'i', 'm', '##uts')\npredicted classes are ['admiration']\nInput text is This internet stranger is also super proud of you! Way to go!!!\nImportant words are:\n('proud', 'super', 'this', 'you', 'stranger', 'go')\npredicted classes are []\nInput text is That’s better:-) now go get em tiger\nImportant words are:\n()\npredicted classes are ['admiration']\nInput text is My jersey has the great number 10 on it!!\nImportant words are:\n('great', 'my', 'it', '!', 'the', '10')\npredicted classes are []\nInput text is And I taught my room was dirty and that I was not taking care of my self. Good job op\nImportant words are:\n()\npredicted classes are ['admiration']\nInput text is I am proud of you random internet stranger, you peopled good today.\nImportant words are:\n('good', 'proud', 'i', 'stranger', 'internet', '##d')\npredicted classes are []\nInput text is That's nothing, before BSE we did horrible things with bone meal animal feed. Cannibal cows.\nImportant words are:\n()\npredicted classes are []\nInput text is Yep. I did this in uni, got mad respect for holding my \"booze\". \nImportant words are:\n()\npredicted classes are ['admiration']\nInput text is You’re doing so good! We’re all so proud!\nImportant words are:\n('good', 'you', 'proud', 'so', 're', 're')\npredicted classes are ['love']\nInput text is I have no shame in saying I still have my blanket from when I was a child. And a favorite [NAME] plushie.\nImportant words are:\n('favorite', 'blanket', '##ie', 'plush', 'i', 'i')\npredicted classes are []\nInput text is We have reached the stage where below 2 million is good. There truly is no bottom to this pit.\nImportant words are:\n()\npredicted classes are ['love']\nInput text is I did! Never moving back, I love my life in the \"big smoke\".\nImportant words are:\n('love', 'did', 'i', '!', 'i', 'back')\npredicted classes are ['admiration']\nInput text is Boy what an accomplishment, so proud!\nImportant words are:\n('proud', 'accomplishment', 'so', ',', 'what', 'an')\npredicted classes are ['neutral']\nInput text is Thats the high life of empowerment.\nImportant words are:\n('of', '.', 'high', '##s', '[CLS]', '[SEP]')\npredicted classes are []\nInput text is I am proud to be racist No one in real life will know this\nImportant words are:\n()\npredicted classes are ['admiration']\nInput text is i’m so proud of you!! good job dude!! :)\nImportant words are:\n('proud', 'good', 'i', 'you', 'so', 'm')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Analyze Relief","metadata":{}},{"cell_type":"code","source":"relief = {\"input\":[], \"labels\":[]}\nfor i,text in enumerate(test_dataset[\"input\"]):\n    out = test_dataset[\"labels\"][i]\n    if out[23] == 1:\n        relief[\"input\"].append(text)\n        relief[\"labels\"].append(out)\n\nprint(len(relief[\"input\"]))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:39:56.355719Z","iopub.execute_input":"2022-11-20T05:39:56.356718Z","iopub.status.idle":"2022-11-20T05:45:16.600486Z","shell.execute_reply.started":"2022-11-20T05:39:56.356682Z","shell.execute_reply":"2022-11-20T05:45:16.599442Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"11\n","output_type":"stream"}]},{"cell_type":"code","source":"data_relief = []\nfor i,text in enumerate(relief[\"input\"]):\n    out = relief[\"labels\"][i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    data_relief.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:45:16.602680Z","iopub.execute_input":"2022-11-20T05:45:16.603051Z","iopub.status.idle":"2022-11-20T05:45:16.622606Z","shell.execute_reply.started":"2022-11-20T05:45:16.603013Z","shell.execute_reply":"2022-11-20T05:45:16.621776Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"attributions, predictions = explainability(data_relief, 16)\npredictions = [pred.detach().cpu().numpy()[0] > 0.32 for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:45:16.623875Z","iopub.execute_input":"2022-11-20T05:45:16.624384Z","iopub.status.idle":"2022-11-20T05:45:56.796543Z","shell.execute_reply.started":"2022-11-20T05:45:16.624349Z","shell.execute_reply":"2022-11-20T05:45:56.795559Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"len(attributions[1][0])","metadata":{"execution":{"iopub.status.busy":"2022-11-20T05:52:34.284603Z","iopub.execute_input":"2022-11-20T05:52:34.285733Z","iopub.status.idle":"2022-11-20T05:52:34.294848Z","shell.execute_reply.started":"2022-11-20T05:52:34.285689Z","shell.execute_reply":"2022-11-20T05:52:34.293900Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"17"},"metadata":{}}]},{"cell_type":"code","source":"#coloring important words\nattribution_colors = []\nfor k, pred in enumerate(predictions):\n    classes = [index_label[index] for index,i in enumerate(pred) if i]\n    print(\"predicted classes are {}\".format(classes))\n    print(\"Input text is {}\".format(relief[\"input\"][k]))\n    print(\"Colored importance is:\")\n    classes = [index for index,i in enumerate(pred) if i]\n    words = []\n    for index in classes:\n        for m in range(6):\n            words.append(data_relief[k][-1][attributions[k][index][m][1]])\n    print(tuple(words))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in [8]:\n    pred = predictions[k]\n    classes = [index_label[index] for index,i in enumerate(pred) if i]\n    print(\"predicted classes are {}\".format(classes))\n    print(\"Input text is {}\".format(relief[\"input\"][k]))\n    print(\"Important words are:\")\n    classes = [index for index,i in enumerate(pred) if i]\n    for index in classes:\n        class_attr = attributions[k][index]\n        word_scores = [0 for _ in range(len(class_attr))]\n        words = [\"\" for _ in range(len(class_attr))]\n        for m in range(len(class_attr)):\n            word_scores[class_attr[m][1]] = class_attr[m][0]\n            words[class_attr[m][1]] = data_relief[k][-1][class_attr[m][1]]\n            \n        print(\" \".join(words[1:-1]))\n        color(word_scores[1:-1], words[1:-1])","metadata":{"execution":{"iopub.status.busy":"2022-11-20T06:17:36.961214Z","iopub.execute_input":"2022-11-20T06:17:36.961594Z","iopub.status.idle":"2022-11-20T06:17:36.978212Z","shell.execute_reply.started":"2022-11-20T06:17:36.961562Z","shell.execute_reply":"2022-11-20T06:17:36.976854Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"predicted classes are ['admiration']\nInput text is This is really helpful to point out!!\nImportant words are:\nthis is really helpful to point out ! !\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<mark style=background-color:#276419>this </mark><mark style=background-color:#fbd8eb>is </mark><mark style=background-color:#aeda7a>really </mark><mark style=background-color:#3d7f1e>helpful </mark><mark style=background-color:#71b038>to </mark><mark style=background-color:#acd977>point </mark><mark style=background-color:#8fc654>out </mark><mark style=background-color:#e9f5d8>! </mark><mark style=background-color:#d6eeb6>! </mark>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Clubbing dataset in sentiment classes","metadata":{}},{"cell_type":"code","source":"sentiment_grouped = {\n    \"positive\": [\"amusement\", \"excitement\", \"joy\", \"love\", \"desire\", \"optimism\", \"caring\", \"pride\", \"admiration\", \"gratitude\", \"relief\", \"approval\"],\n    \"negative\": [\"fear\", \"nervousness\", \"remorse\", \"embarrassment\", \"disappointment\", \"sadness\", \"grief\", \"disgust\", \"anger\", \"annoyance\", \"disapproval\"],\n    \"ambiguous\": [\"realization\", \"surprise\", \"curiosity\", \"confusion\"],\n    \"neutral\": [\"neutral\"]\n}\nindex_label_sentiment = {0: \"positive\", 1: \"negative\", 2: \"ambiguous\", 3: \"neutral\"}\nlabel_index_sentiment = {\"positive\": 0, \"negative\": 1, \"ambiguous\": 2, \"neutral\": 3}\nlabel_list_sentiment = [\"positive\", \"negative\", \"ambiguous\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-11-16T10:11:09.008951Z","iopub.execute_input":"2022-11-16T10:11:09.009354Z","iopub.status.idle":"2022-11-16T10:11:09.016044Z","shell.execute_reply.started":"2022-11-16T10:11:09.009321Z","shell.execute_reply":"2022-11-16T10:11:09.014996Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_sentiment(label, sentiment_grouped, label_index_sentiment):\n    for key, value in sentiment_grouped.items():\n        if label in value:\n            return label_index_sentiment[key]\n    return label_index_sentiment[\"neutral\"]\n\n# changing labels of test set\ntest_sentiment = {\"input\":[], \"labels\":[]}\nfor i,text in enumerate(test_dataset[\"input\"]):\n    output = test_dataset[\"labels\"][i]\n    new_out = [0 for _ in range(4)]\n    for j, out in enumerate(output):\n        if out == 1:\n            sentiment = get_sentiment(index_label[j], sentiment_grouped, label_index_sentiment)\n            new_out[sentiment] = 1\n    \n    test_sentiment[\"input\"].append(text)\n    test_sentiment[\"labels\"].append(new_out)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T10:11:16.742692Z","iopub.execute_input":"2022-11-16T10:11:16.743078Z","iopub.status.idle":"2022-11-16T10:16:58.206945Z","shell.execute_reply.started":"2022-11-16T10:11:16.743046Z","shell.execute_reply":"2022-11-16T10:16:58.205938Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntest_sentiment_dataset = Dataset.from_dict(test_sentiment)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T10:16:58.208705Z","iopub.execute_input":"2022-11-16T10:16:58.209069Z","iopub.status.idle":"2022-11-16T10:16:58.224995Z","shell.execute_reply.started":"2022-11-16T10:16:58.209033Z","shell.execute_reply":"2022-11-16T10:16:58.224126Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test_sent_obj = LoadData(\n        test_sentiment_dataset,\n        tokenizer,\n        parameters[\"max_source_length\"]\n    )\ntest_loader = DataLoader(test_sent_obj, shuffle=True, batch_size=parameters[\"test_bs\"])\npredictions, labels = compute_test_outputs(model, test_loader, tokenizer, device, label_list_sentiment, index_label_sentiment)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T14:44:26.195284Z","iopub.execute_input":"2022-11-14T14:44:26.195645Z","iopub.status.idle":"2022-11-14T14:46:00.657324Z","shell.execute_reply.started":"2022-11-14T14:44:26.195617Z","shell.execute_reply":"2022-11-14T14:46:00.656305Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"predictions_sent = []\nfor i,pred in enumerate(predictions):\n    new_out = [0 for _ in range(4)]\n    for j, out in enumerate(pred):\n        if out > 0.32:\n            sentiment = get_sentiment(index_label[j], sentiment_grouped, label_index_sentiment)\n            new_out[sentiment] = 1\n    \n    predictions_sent.append(new_out)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T14:48:45.581311Z","iopub.execute_input":"2022-11-14T14:48:45.581671Z","iopub.status.idle":"2022-11-14T14:48:45.820437Z","shell.execute_reply.started":"2022-11-14T14:48:45.581643Z","shell.execute_reply":"2022-11-14T14:48:45.819453Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"precisions, recalls, fscores = compute_metrics_allemotions(predictions_sent, labels, label_list_sentiment, index_label_sentiment)\nprint(\"Precision, Recall and Fscores for all labels are \")\n\nprecision, recall, fscore = 0, 0, 0\nfor label in label_list_sentiment:\n    precision += precisions[label]\n    recall += recalls[label]\n    fscore += fscores[label]\n    print(\"Emotion {}: precision: {}, recall: {}, fscore: {}\".format(label, precisions[label], \n                                                                     recalls[label], fscores[label]))\n\nprint(\"Macro precision: {}, Macro recall: {}, Macro fscore: {}\".format(precision/4, recall/4, fscore/4))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T14:50:50.281409Z","iopub.execute_input":"2022-11-14T14:50:50.281759Z","iopub.status.idle":"2022-11-14T14:50:50.367010Z","shell.execute_reply.started":"2022-11-14T14:50:50.281731Z","shell.execute_reply":"2022-11-14T14:50:50.365884Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"1st prediction [0, 1, 0, 0]\nPrecision, Recall and Fscores for all labels are \nEmotion positive: precision: 0.8766446887804447, recall: 0.7599809524723882, fscore: 0.8141050018001232\nEmotion negative: precision: 0.8385415210865416, recall: 0.38272580168575265, fscore: 0.5255281845533213\nEmotion ambiguous: precision: 0.6845636052430414, recall: 0.4519940248162445, fscore: 0.5444359866926374\nEmotion neutral: precision: 0.7112117377569798, recall: 0.7028539058279851, fscore: 0.7069581276009392\nMacro precision: 0.7777403882167517, Macro recall: 0.5743886712005927, Macro fscore: 0.6477568251617553\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Analysing admiration with Perturbations in Input","metadata":{}},{"cell_type":"code","source":"admiration = {\"input\":[], \"labels\":[]}\ncnt = 0\nfor i,text in enumerate(test_dataset[\"input\"]):\n    out = test_dataset[\"labels\"][i]\n    if out[0] == 1:\n        cnt += 1\n        admiration[\"input\"].append(text)\n        admiration[\"labels\"].append(out)\n    if cnt == 30: break\n\nprint(len(admiration[\"input\"]))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T11:00:57.966254Z","iopub.execute_input":"2022-11-16T11:00:57.966628Z","iopub.status.idle":"2022-11-16T11:01:23.482954Z","shell.execute_reply.started":"2022-11-16T11:00:57.966597Z","shell.execute_reply":"2022-11-16T11:01:23.481917Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"30\n","output_type":"stream"}]},{"cell_type":"code","source":"data_admiration = []\nfor i,text in enumerate(admiration[\"input\"]):\n    out = admiration[\"labels\"][i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    data_admiration.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T11:01:33.784254Z","iopub.execute_input":"2022-11-16T11:01:33.784651Z","iopub.status.idle":"2022-11-16T11:01:33.835607Z","shell.execute_reply.started":"2022-11-16T11:01:33.784617Z","shell.execute_reply":"2022-11-16T11:01:33.834779Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"attributions, predictions = explainability(data_admiration, 16)\npredictions = [pred.detach().cpu().numpy()[0] > 0.32 for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-11-16T11:01:58.743518Z","iopub.execute_input":"2022-11-16T11:01:58.744131Z","iopub.status.idle":"2022-11-16T11:02:55.549308Z","shell.execute_reply.started":"2022-11-16T11:01:58.744090Z","shell.execute_reply":"2022-11-16T11:02:55.548323Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for k, pred in enumerate(predictions):\n    classes = [index_label[index] for index,i in enumerate(pred) if i]\n    print(\"predicted classes are {}\".format(classes))\n    print(\"Input text is {}\".format(admiration[\"input\"][k]))\n    print(\"Important words are:\")\n    classes = [index for index,i in enumerate(pred) if i]\n    words = []\n    for index in classes:\n        for m in range(6):\n            words.append(data_admiration[k][-1][attributions[k][index][m][1]])\n    print(tuple(words))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T10:50:30.279513Z","iopub.execute_input":"2022-11-16T10:50:30.279883Z","iopub.status.idle":"2022-11-16T10:50:30.289560Z","shell.execute_reply.started":"2022-11-16T10:50:30.279853Z","shell.execute_reply":"2022-11-16T10:50:30.288269Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"predicted classes are ['admiration']\nInput text is It's wonderful because it's awful. At not with.\nImportant words are:\n('wonderful', 'awful', 'it', 's', 's', 'with')\npredicted classes are ['admiration', 'curiosity']\nInput text is It's great that you're a recovering addict, that's cool. Have you ever tried DMT?\nImportant words are:\n('great', 'cool', 'it', 's', 'that', 're', '?', \"'\", 'recovering', 'd', ',', 'tried')\npredicted classes are ['love']\nInput text is This guy is a little turd but I love him so dearly. I'll pass on your kisses :)\nImportant words are:\n('love', 'i', 'i', '##rd', 'so', 'this')\npredicted classes are ['amusement']\nInput text is in what universe? lol the mr. blue sky cover is one of the best on the album imo.\nImportant words are:\n('lo', '##l', 'universe', 'in', 'album', 'best')\npredicted classes are ['admiration', 'amusement']\nInput text is Lol looks delicious\nImportant words are:\n('delicious', 'looks', '[CLS]', '[SEP]', '##l', 'lo', 'lo', '##l', 'looks', '[CLS]', '[SEP]', 'delicious')\npredicted classes are ['admiration', 'approval']\nInput text is Some great advice right here!!!!!! I forgot about this\nImportant words are:\n('great', 'advice', 'some', 'i', '!', '!', 'right', 'advice', '!', 'here', 'i', 'great')\npredicted classes are ['admiration']\nInput text is New Haven has tons of locations that have strong cyber punk-y vibes at night!\nImportant words are:\n('locations', 'strong', 'tons', 'new', '!', 'cyber')\npredicted classes are ['admiration', 'love']\nInput text is Fucking love [NAME]. [NAME] best couple don't @ me\nImportant words are:\n('best', 'fucking', 'love', \"'\", 't', '.', 'love', 'me', '[', ']', 't', '[')\npredicted classes are ['admiration']\nInput text is I'm on board the Saints hype train. [NAME] is an amazing QB.\nImportant words are:\n('amazing', 'name', 'h', 'm', 'saints', '##ype')\npredicted classes are ['admiration']\nInput text is You have a nice bro.\nImportant words are:\n('nice', 'you', 'bro', '.', 'a', '[CLS]')\npredicted classes are []\nInput text is [NAME] stopped napping at 18 months. We’ve been implementing quiet time since. It’s wondrous.\nImportant words are:\n()\npredicted classes are ['admiration']\nInput text is Proud of you.\nImportant words are:\n('proud', 'of', 'you', '.', '[CLS]', '[SEP]')\npredicted classes are ['admiration']\nInput text is Nah, he's much smarter than I am about soccer.\nImportant words are:\n('smarter', 'soccer', '.', 'much', 'he', 'than')\npredicted classes are ['admiration']\nInput text is That’s cool. I had a few baby teeth dipped in gold and made into charms for a bracelet. Gorgeous work.\nImportant words are:\n('gorgeous', 'cool', 'that', 'gold', 'dipped', 'work')\npredicted classes are ['admiration']\nInput text is Well done dude! Seriously :) But keep it up, don't lose this momentum.\nImportant words are:\n('done', 'well', '!', 'keep', ')', 'don')\npredicted classes are ['admiration']\nInput text is thats amazing!! Congrats!\nImportant words are:\n('amazing', 'that', 'cong', '##rat', '!', '##s')\n","output_type":"stream"}]},{"cell_type":"code","source":"# removing certain words and checking\n# original text = Well done dude! Seriously :) But keep it up, don't lose this momentum.\ntexts = [\"WSeriously :) But keep it up, don't lose this momentum.\"]\nlabels = [[0 for i in range(28)]]\nlabels[0][0] = 1\ndata_peturbe = []\nfor i,text in enumerate(texts):\n    out = labels[i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    data_peturbe.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))\nattributions_peturbe, predictions_peturbe = explainability(data_peturbe, 16)\nprint(predictions_peturbe)\npredictions_peturbe = [pred.detach().cpu().numpy()[0] > 0.32 for pred in predictions_peturbe]\nfor k, pred in enumerate(predictions_peturbe):\n    classes = [index_label[index] for index,i in enumerate(pred) if i]\n    print(\"predicted classes are {}\".format(classes))\n    print(\"Input text is {}\".format(texts[k]))\n    print(\"Important words are:\")\n    classes = [index for index,i in enumerate(pred) if i]\n    words = []\n    for index in classes:\n        for m in range(6):\n            words.append(data_peturbe[k][-1][attributions_peturbe[k][index][m][1]])\n    print(tuple(words))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T12:03:40.780826Z","iopub.execute_input":"2022-11-16T12:03:40.781420Z","iopub.status.idle":"2022-11-16T12:03:44.286727Z","shell.execute_reply.started":"2022-11-16T12:03:40.781367Z","shell.execute_reply":"2022-11-16T12:03:44.285702Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[tensor([[0.0357, 0.0025, 0.0170, 0.0703, 0.1754, 0.1414, 0.0047, 0.0032, 0.0090,\n         0.0679, 0.0820, 0.0118, 0.0055, 0.0083, 0.0247, 0.0815, 0.0055, 0.0186,\n         0.0014, 0.0253, 0.0699, 0.0221, 0.0770, 0.0766, 0.0062, 0.0204, 0.0081,\n         0.1479]], device='cuda:0', grad_fn=<SigmoidBackward0>)]\npredicted classes are []\nInput text is Seriously :) But keep it up, don't lose this momentum.\nImportant words are:\n()\n","output_type":"stream"}]},{"cell_type":"code","source":"data_admiration[0][-1]","metadata":{"execution":{"iopub.status.busy":"2022-11-16T11:04:32.969141Z","iopub.execute_input":"2022-11-16T11:04:32.969500Z","iopub.status.idle":"2022-11-16T11:04:32.976272Z","shell.execute_reply.started":"2022-11-16T11:04:32.969472Z","shell.execute_reply":"2022-11-16T11:04:32.975331Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'it',\n \"'\",\n 's',\n 'wonderful',\n 'because',\n 'it',\n \"'\",\n 's',\n 'awful',\n '.',\n 'at',\n 'not',\n 'with',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}