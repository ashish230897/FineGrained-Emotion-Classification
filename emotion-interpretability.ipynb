{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install captum","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:02:43.143228Z","iopub.execute_input":"2022-11-10T17:02:43.143599Z","iopub.status.idle":"2022-11-10T17:02:56.470949Z","shell.execute_reply.started":"2022-11-10T17:02:43.143519Z","shell.execute_reply":"2022-11-10T17:02:56.469829Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting captum\n  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from captum) (1.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from captum) (1.21.6)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from captum) (3.5.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->captum) (4.1.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (1.4.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (21.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (2.8.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->captum) (9.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.15.0)\nInstalling collected packages: captum\nSuccessfully installed captum-0.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nimport torch\nfrom datasets import Dataset\nimport transformers\nfrom transformers import (\n  AdamW,\n  BertConfig,\n  BertModel,\n  BertTokenizer,\n  DistilBertTokenizer,\n  DistilBertModel,\n  DistilBertForSequenceClassification,\n  BertForSequenceClassification)\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport os\nimport captum\nfrom captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\nfrom captum.attr import visualization as viz\nfrom captum.attr import LayerConductance, LayerIntegratedGradients\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:04.944156Z","iopub.execute_input":"2022-11-10T17:03:04.944557Z","iopub.status.idle":"2022-11-10T17:03:12.272890Z","shell.execute_reply.started":"2022-11-10T17:03:04.944523Z","shell.execute_reply":"2022-11-10T17:03:12.271768Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"finegrained_sentiments_dict = {\n\"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n\"disgust\": [\"disgust\"],\n\"fear\": [\"fear\", \"nervousness\"],\n\"joy\": [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",  \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"],\n\"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n\"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]\n}","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:12.275286Z","iopub.execute_input":"2022-11-10T17:03:12.276350Z","iopub.status.idle":"2022-11-10T17:03:12.282283Z","shell.execute_reply.started":"2022-11-10T17:03:12.276310Z","shell.execute_reply":"2022-11-10T17:03:12.280948Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!ls ../","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:12.283600Z","iopub.execute_input":"2022-11-10T17:03:12.284631Z","iopub.status.idle":"2022-11-10T17:03:13.260813Z","shell.execute_reply.started":"2022-11-10T17:03:12.284597Z","shell.execute_reply":"2022-11-10T17:03:13.259726Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"input  lib  working\n","output_type":"stream"}]},{"cell_type":"code","source":"DATA_DIR = \"../input/emotionclss/\"\ntrain = {\"input\": [], \"labels\": []}\ndev = {\"input\": [], \"labels\": []}\ntest = {\"input\": [], \"labels\": []}\n\nwith open(DATA_DIR + \"train.tsv\") as file:\n    tsv_file = csv.reader(file, delimiter=\"\\t\") \n    for line in tsv_file:\n        train[\"input\"].append(line[0])\n        labels = line[1].split(\",\")\n        one_hot = [0 for i in range(28)]\n        for label in labels:\n            one_hot[int(label)] = 1\n        train[\"labels\"].append(one_hot)\n\nwith open(DATA_DIR + \"dev.tsv\") as file:\n    tsv_file = csv.reader(file, delimiter=\"\\t\") \n    for line in tsv_file:\n        dev[\"input\"].append(line[0])\n        labels = line[1].split(\",\")\n        one_hot = [0 for i in range(28)]\n        for label in labels:\n            one_hot[int(label)] = 1\n        dev[\"labels\"].append(one_hot)\n\nwith open(DATA_DIR + \"test.tsv\") as file:\n    tsv_file = csv.reader(file, delimiter=\"\\t\") \n    for line in tsv_file:\n        test[\"input\"].append(line[0])\n        labels = line[1].split(\",\")\n        one_hot = [0 for i in range(28)]\n        for label in labels:\n            one_hot[int(label)] = 1\n        test[\"labels\"].append(one_hot)\n        \nprint(\"Number of train examples are {}\".format(len(train[\"input\"])))\nprint(\"Number of dev examples are {}\".format(len(dev[\"input\"])))\nprint(\"Number of test examples are {}\".format(len(test[\"input\"])))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:15.642818Z","iopub.execute_input":"2022-11-10T17:03:15.643198Z","iopub.status.idle":"2022-11-10T17:03:16.141573Z","shell.execute_reply.started":"2022-11-10T17:03:15.643165Z","shell.execute_reply":"2022-11-10T17:03:16.140525Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of train examples are 43410\nNumber of dev examples are 5426\nNumber of test examples are 5427\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating higgingface datasets\ntrain_dataset = Dataset.from_dict(train)\ndev_dataset = Dataset.from_dict(dev)\ntest_dataset = Dataset.from_dict(test)\n\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:18.191856Z","iopub.execute_input":"2022-11-10T17:03:18.192545Z","iopub.status.idle":"2022-11-10T17:03:18.413318Z","shell.execute_reply.started":"2022-11-10T17:03:18.192494Z","shell.execute_reply":"2022-11-10T17:03:18.412303Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input', 'labels'],\n    num_rows: 43410\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass LoadData(Dataset):\n    \"\"\"\n    Using this since dataloader expects map-style dataset objects\n    \n    \"\"\"\n    \n    def __init__(\n        self, dataset, tokenizer, source_length):\n        \"\"\"\n        Initializes a Dataset class\n\n        Args:\n            dataset (Dataset object): Input Dataset\n            tokenizer (Tokenizer object): Transformer tokenizer\n            source_length (int): Max length of source text\n        \"\"\"\n        \n        self.tokenizer = tokenizer\n        self.data = dataset\n        self.source_length = source_length\n        self.source_text = self.data[\"input\"]\n        self.target_labels = self.data[\"labels\"]\n\n    def __len__(self):\n        return len(self.target_labels)\n\n    def __getitem__(self, index):\n        \"\"\"\n        return input ids, attention masks and target ids\n        \n        \"\"\"\n        source_text = str(self.source_text[index])\n        target_label = self.target_labels[index]\n\n        # cleaning data so as to ensure data is in string type\n        source_text = \" \".join(source_text.split())\n\n        source = self.tokenizer.__call__(\n            [source_text],\n            max_length=self.source_length,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        \n        target = torch.tensor(target_label)\n\n        source_ids = source[\"input_ids\"].squeeze()\n        source_mask = source[\"attention_mask\"].squeeze()\n\n        return {\n            \"source_ids\": source_ids.to(dtype=torch.long),\n            \"source_mask\": source_mask.to(dtype=torch.long),\n            \"target\": target.squeeze().to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:22.030291Z","iopub.execute_input":"2022-11-10T17:03:22.030974Z","iopub.status.idle":"2022-11-10T17:03:22.042038Z","shell.execute_reply.started":"2022-11-10T17:03:22.030938Z","shell.execute_reply":"2022-11-10T17:03:22.040753Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#joeddav/distilbert-base-uncased-go-emotions-student\nparameters = {\"model\": \"bhadresh-savani/bert-base-go-emotion\",  # model_type: t5-base/t5-large\n    \"train_bs\": 8,  # training batch size\n    \"val_bs\": 10,  # validation batch size\n    \"test_bs\": 15,\n    \"epochs\": 3,  # number of training epochs\n    \"lr\": 6e-4,  # learning rate\n    \"wd\": 0.0001,\n    \"max_source_length\": 512,  # max length of source text\n    \"SEED\": 42,\n    \"out_dir\": \"./\",\n    \"hidden_size\": 768,\n    \"num_classes\": 28}\n\nindex_label = {0:\"admiration\", 1:\"amusement\", 2:\"anger\", 3:\"annoyance\", 4:\"approval\", 5:\"caring\", 6:\"confusion\",\n            7:\"curiosity\", 8:\"desire\", 9:\"disappointment\", 10:\"disapproval\", 11:\"disgust\", 12:\"embarrassment\",\n            13:\"excitement\", 14:\"fear\", 15:\"gratitude\", 16:\"grief\", 17:\"joy\", 18:\"love\", 19:\"nervousness\",\n            20:\"optimism\", 21:\"pride\", 22:\"realization\", 23:\"relief\", 24:\"remorse\", 25:\"sadness\",\n            26:\"surprise\", 27:\"neutral\"}\nlabel_list = list(index_label.values())","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:23.156716Z","iopub.execute_input":"2022-11-10T17:03:23.157063Z","iopub.status.idle":"2022-11-10T17:03:23.164638Z","shell.execute_reply.started":"2022-11-10T17:03:23.157033Z","shell.execute_reply":"2022-11-10T17:03:23.163565Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_test_outputs(model, test_dataloader, tokenizer, device, label_list, index_label):\n    predictions = []\n    labels = []\n    \n    with torch.no_grad():\n        steps = 0\n        for test_batch in test_dataloader:\n            y = test_batch['target'].to(device, dtype = torch.float32)\n            ids = test_batch['source_ids'].to(device, dtype = torch.long)\n            mask = test_batch['source_mask'].to(device, dtype = torch.long)\n\n            output = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n            \n            output = output[\"logits\"]\n            output = torch.sigmoid(output)\n            \n            predictions.extend(output.detach().cpu().numpy())\n            labels.extend(y.detach().cpu().numpy())\n            if steps == 5: break\n    \n    return predictions, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:28.509526Z","iopub.execute_input":"2022-11-10T17:03:28.509951Z","iopub.status.idle":"2022-11-10T17:03:28.523326Z","shell.execute_reply.started":"2022-11-10T17:03:28.509913Z","shell.execute_reply":"2022-11-10T17:03:28.522338Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"cuda =  torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n\ntokenizer = DistilBertTokenizer.from_pretrained(parameters[\"model\"])\nmodel = BertForSequenceClassification.from_pretrained(parameters[\"model\"])\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:32.324119Z","iopub.execute_input":"2022-11-10T17:03:32.324508Z","iopub.status.idle":"2022-11-10T17:03:51.497769Z","shell.execute_reply.started":"2022-11-10T17:03:32.324469Z","shell.execute_reply":"2022-11-10T17:03:51.496603Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f32950ae479407e9148645099eb15c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe93ef3bf3bd47dbab2f35f441879636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/333 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1af154ee97a4aeab2d616abc02b418a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38583436e495438f8e672f80212a64e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6a7ef33ca5404ca17c2ddfcea3820e"}},"metadata":{}}]},{"cell_type":"code","source":"ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\nsep_token_id = tokenizer.sep_token_id # A token used as a separator at the end of the text.\ncls_token_id = tokenizer.cls_token_id # A token used for prepending to the word sequence","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:03:55.307093Z","iopub.execute_input":"2022-11-10T17:03:55.307685Z","iopub.status.idle":"2022-11-10T17:03:55.313274Z","shell.execute_reply.started":"2022-11-10T17:03:55.307639Z","shell.execute_reply":"2022-11-10T17:03:55.312333Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device):\n    text_ids = tokenizer.encode(text, add_special_tokens=False)\n\n    # construct input token ids\n    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n\n    # construct reference token ids \n    ref_input_ids = [cls_token_id] + [ref_token_id] * (len(input_ids)-2) + [sep_token_id]\n\n    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(input_ids)\n\ndef construct_attention_mask(input_ids):\n    return torch.ones_like(input_ids)\n\ndef predict(inputs, attention_mask=None):\n    output = model(inputs, attention_mask=attention_mask, )\n    output = output[\"logits\"]\n    output = torch.sigmoid(output)\n    return output\n\ndef forward_func(inputs, i, device, attention_mask=None):\n    pred = predict(inputs,\n                   attention_mask=attention_mask)\n    #return pred.max(1).values\n    pred = torch.index_select(pred, 1, torch.tensor([i], device=device))\n    return pred\n\ndef summarize_attributions(attributions):\n    attributions = attributions.sum(dim=-1).squeeze(0)\n    attributions = attributions / torch.norm(attributions)\n    return attributions","metadata":{"execution":{"iopub.status.busy":"2022-11-10T17:43:03.205480Z","iopub.execute_input":"2022-11-10T17:43:03.205831Z","iopub.status.idle":"2022-11-10T17:43:03.215721Z","shell.execute_reply.started":"2022-11-10T17:43:03.205803Z","shell.execute_reply":"2022-11-10T17:43:03.214690Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# preparing data\ndata = []\nfor i,text in enumerate(test_dataset[\"input\"][0:100]):\n    out = test_dataset[\"labels\"][i]\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, device)\n    attention_mask = construct_attention_mask(input_ids)\n    out = torch.tensor([out], device=device)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n\n    #print(len(out[0]), len(input_ids[0]), len(ref_input_ids[0]), len(attention_mask[0]))\n    data.append((out, input_ids, ref_input_ids, attention_mask, all_tokens))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:25:55.337941Z","iopub.execute_input":"2022-11-10T18:25:55.338280Z","iopub.status.idle":"2022-11-10T18:26:01.437252Z","shell.execute_reply.started":"2022-11-10T18:25:55.338251Z","shell.execute_reply":"2022-11-10T18:26:01.436321Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"lig = LayerIntegratedGradients(forward_func, model.bert.embeddings)  # embeddings is the first layer\nattributions = []\npredictions = []\nfor j,sample in enumerate(data[0:4]):\n    pred = predict(sample[1], attention_mask=sample[3])\n    \n    classes = []\n    for i in range(0,28):\n        attribution, delta = lig.attribute(inputs=sample[1],\n                                      baselines=sample[2],\n                                      additional_forward_args=(i, device, sample[3]),\n                                      return_convergence_delta=True)\n        attribution = summarize_attributions(attribution).detach().cpu().numpy()\n        attribution = [(attr,k) for k,attr in enumerate(attribution)]\n        attribution.sort(key=(lambda x: x[0]), reverse=True)\n        classes.append(attribution)\n    attributions.append(classes)\n    predictions.append(pred)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:48:24.739297Z","iopub.execute_input":"2022-11-10T18:48:24.739822Z","iopub.status.idle":"2022-11-10T18:48:38.986306Z","shell.execute_reply.started":"2022-11-10T18:48:24.739787Z","shell.execute_reply":"2022-11-10T18:48:38.985363Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"print(len(attributions[1][0]))\nprint(len(data[1][-1]))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:49:49.002120Z","iopub.execute_input":"2022-11-10T18:49:49.002487Z","iopub.status.idle":"2022-11-10T18:49:49.008173Z","shell.execute_reply.started":"2022-11-10T18:49:49.002455Z","shell.execute_reply":"2022-11-10T18:49:49.007057Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"16\n16\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Predicted label is:\", index_label[np.argmax(predictions[3][0].detach().cpu().numpy())], np.argmax(predictions[3][0].detach().cpu().numpy()))\nprint(\"printing more valuable 3 words for each class:\")\nfor i in range(28):\n    print(\"Now we have class {}!!!!\".format(index_label[i]))\n    words = []\n    for k in range(5):\n        words.append(data[3][-1][attributions[3][i][k][1]])\n    print(tuple(words))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:50:39.731184Z","iopub.execute_input":"2022-11-10T18:50:39.731552Z","iopub.status.idle":"2022-11-10T18:50:39.741174Z","shell.execute_reply.started":"2022-11-10T18:50:39.731514Z","shell.execute_reply":"2022-11-10T18:50:39.740051Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Predicted label is: gratitude 15\nprinting more valuable 3 words for each class:\nNow we have class admiration!!!!\n(',', \"'\", '!', 'thank', 'today')\nNow we have class amusement!!!!\n('i', '!', 'me', \"'\", ',')\nNow we have class anger!!!!\n('t', 'me', 'didn', 'teaching', '!')\nNow we have class annoyance!!!!\n('t', 'didn', 'me', \"'\", '!')\nNow we have class approval!!!!\n(\"'\", 'know', 'you', 'today', ',')\nNow we have class caring!!!!\n('you', 'thank', \"'\", ',', 'for')\nNow we have class confusion!!!!\n('know', 'didn', 't', 'something', 'i')\nNow we have class curiosity!!!!\n('know', 'didn', 'something', 'me', 'for')\nNow we have class desire!!!!\n('i', 'something', 'for', 'you', 'today')\nNow we have class disappointment!!!!\n('t', 'didn', 'me', 'teaching', 'for')\nNow we have class disapproval!!!!\n('t', \"'\", 'didn', 'teaching', '[CLS]')\nNow we have class disgust!!!!\n('t', 'didn', 'me', \"'\", 'teaching')\nNow we have class embarrassment!!!!\n('didn', 'me', 'i', 't', 'teaching')\nNow we have class excitement!!!!\n('!', 'me', 'something', 'know', \"'\")\nNow we have class fear!!!!\n('me', 'something', 'for', 'didn', 'teaching')\nNow we have class gratitude!!!!\n('thank', \"'\", '!', 'know', 'you')\nNow we have class grief!!!!\n('me', 'for', 'teaching', 'that', ',')\nNow we have class joy!!!!\n('!', \"'\", 'me', 'something', 'know')\nNow we have class love!!!!\n('!', ',', 'me', 'today', \"'\")\nNow we have class nervousness!!!!\n('me', 'something', 'didn', 'for', 'i')\nNow we have class optimism!!!!\n('you', 'something', 'know', \"'\", 'today')\nNow we have class pride!!!!\n(\"'\", ',', '!', 'today', 'you')\nNow we have class realization!!!!\n('know', 't', 'i', \"'\", 'didn')\nNow we have class relief!!!!\n(\"'\", '!', ',', 'me', 'you')\nNow we have class remorse!!!!\n('didn', 'i', 't', 'me', 'teaching')\nNow we have class sadness!!!!\n('me', 'for', 'teaching', 'didn', 'that')\nNow we have class surprise!!!!\n('t', '!', 'i', 'me', 'know')\nNow we have class neutral!!!!\n('today', '!', 'know', 'teaching', '[CLS]')\n","output_type":"stream"}]},{"cell_type":"code","source":"start_position_vis = viz.VisualizationDataRecord(\n                        attributions_start_sum,\n                        torch.max(pred),\n                        torch.argmax(pred),\n                        torch.argmax(pred),\n                        str(14),\n                        attributions_start_sum.sum(),\n                        all_tokens,\n                        delta_start)\n\nprint('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\nviz.visualize_text([start_position_vis])","metadata":{"execution":{"iopub.status.busy":"2022-11-09T17:31:08.446372Z","iopub.execute_input":"2022-11-09T17:31:08.447045Z","iopub.status.idle":"2022-11-09T17:31:08.465652Z","shell.execute_reply.started":"2022-11-09T17:31:08.447009Z","shell.execute_reply":"2022-11-09T17:31:08.464631Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[1m Visualizations For Start Position \u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>20</b></text></td><td><text style=\"padding-right:2em\"><b>20 (0.42)</b></text></td><td><text style=\"padding-right:2em\"><b>14</b></text></td><td><text style=\"padding-right:2em\"><b>1.40</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fan                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> good                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luck                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> guys                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interesting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watch                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>20</b></text></td><td><text style=\"padding-right:2em\"><b>20 (0.42)</b></text></td><td><text style=\"padding-right:2em\"><b>14</b></text></td><td><text style=\"padding-right:2em\"><b>1.40</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fan                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> good                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luck                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> guys                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interesting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watch                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}